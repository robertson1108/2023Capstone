---
title: "Pov_Capstone"
author: "Jamie Robertson"
date: "2023-02-20"
output: html_document
subtitle: Capstone project re The Poverello Center
bibliography: cap.bib
nocite: '@*'

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(show.signif.stars = FALSE)
library(knitr)
library(NHANES)
library(readr)
library(mosaic)
library(heplots)
library(psych)
library(car)
library(Sleuth3)
library(coneproj)
library(MuMIn)
library(rpart)
library(randomForest)
library(tidymodels)
library(car)

knitr::write_bib(c("base","rmarkdown","knitr", #don't change these
                   "NHANES","mosaic","psych","car","MuMIn"), #project specific before the close parenthesis
                   "cap.bib", width=60)

knitr::opts_chunk$set(echo = TRUE)
```


## Introduction/Background

The data source: Poverello Center
Data dictionary for HMIS data: https://files.hudexchange.info/resources/documents/HMIS-Data-Dictionary.pdf
The point of this analysis is to use benefit The Poverello Center (The Pov). This is a detail review of the data and exploratory analysis. The Pov hopes to utilize their data to identify individuals who are likely to be in the top percentile of clients. 


##Research questions

What are the most important variables to use in a model to explain high service counts a person has for services received from The Poverello Center?

Predictive risk modeling for top 20% users of homeless services in Missoula Montana. 

```{r show_col_types = FALSE}
#Read in the data

d.pov <- read_csv("C:\\Users\\jamie\\OneDrive\\Documents\\Capstone\\LRdata.csv", show_col_types = FALSE)

```
##Overview

The HMIS data from The Pov contains many demographic variables including vet status, race, income, weather or not the client holds insurance, if the client is handicapped, etc. The number of unique individuals included in the data started as 2383. There were over 50,000 service transactions. These observations were collected November 2021 through December 2022 in Missoula, Montana. The variables used in this analysis are listed explained more thoroughly in Appendix I of the associated paper. 
```{r}
head(d.pov)
```

These variables may be useful in a linear regression. The sum of services provided to a client is the dependent variable. In the cell below the counts of all services recieved by an individual and added to the dataframe. 

```{r add_counts, echo=FALSE}

d.pov <- d.pov %>% 
  mutate(totserv=efood_count+wfood_count+eother_count+wother_count)

hist(d.pov$totserv, caption="Graph 1. Histogram for summed service counts by individual")

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


##Context

The highest percentile will be the clients who stay in a homeless or at risk for a longer period of time. The top 10% consumed 45.5% of the services of the Pov in the time frame provided. 

```{r}

quantile(d.pov$totserv, probs = seq(.1, .9, by =.1))

```


##Best Practices for Data Cleaning

The data came in 4 files, a set of two from each location of the Pov. One set from the Winter Shelter which operation depends on the weather but mostly is open from November to April, one set from the Emergency Shelter that is open all year. The client files were combined and duplicates were removed in excel. A column to calculate age based on the date of birth was also entered in excel.

Many entries were missing portions of the data. After matching the Client_ID with transactions. I need to deal with so many null values. Luckily, many of the variables in the data are binary variables. Using Google Big Query and SQL were used to combine the client data with the food transaction data from the two locations. The data was transformed there to 1's and 0's. This approach turned missing data to false for the following variables: female, male, transgender, veteran status, race_white, race_black, race_native, race_asian, ethnically hispanic, domestic violence survivor, covered by health insurance, has a disabling condition, if they have stayed one night, two to six nights, one week to a month, one to three months, three months to a year, or a year and over, if they are the head of household, reported a second race, has income from any source, or recieves a non-cash benefit.  This code can be found in the appendix of the associated paper. 

The variable that holds the total number of months an individual has been homeless contains many null values. This variable was removed and complete cases were consolidated. There are 347 complete rows of data.  



```{r}

#Deal with null values
d <-   select(d.pov, Age_,
         females, 
         transgender, 
         males, 
         vet, 
         race_white, 
         race_black, 
         race_native,
         race_asian, 
         eth_hispanic, 
         dviolencesurvivor, 
         insured, 
         disabled, 
         lenstay_1, 
         lenstay_2to6, 
         lenstay_1to3mo, 
         lenstay_wktomo, 
         lenstay_1yrplus, 
         lenstay_3to12mo, 
         income, 
         headofhouse, 
         race2_reported, 
         hasincome, 
         hasbenefit, 
         totserv) 
         
d.complete.sum <- complete.cases(d)
summary(d.complete.sum)

d.complete <- d[complete.cases(d),]

```    


#### Data Summaries

As noted above and described below in Table 1, the quantitative variables are age,.... The standard deviation of TotServ is 1.062, the mean is 5.053, and the shape is right skewed. The count of catagorical variables

```{r}
variableNames <- c("Age_", "months_from_start", "income","efood_count","eother_count",
                   "wfood_count","wother_count","totserv")
kable(describe(d.pov[,variableNames])[,-c(1,6:7,11:12)], digits=3,
      caption="Table 1. Summary Statistics of Quantitative Variables")
```



##Code explanations



The point of this analysis is to use Akaike's "An Information Criterion" method to select the appropriate model to answer the research question.  This method is also known as AIC model selection.  
The point of the analysis is to determine the most important variables to use in a model to explain the amount of services used by a client. This paper reviews some descriptive statistics from the data that utilizes AIC to pick the best model.


Total Count of Services will be the response variable.  This variable will be identified as TotServ and measures a client consumption of services. This is a quantitative variable. Veteran status is included as vet ...


```{r}

kable(tally(d.pov$hasbenefit), caption = "Table 2: Count of clients with benefits")
#tried to change names
#kable(tally(d.pov$hasbenefit, ""  ), caption = "Table 2: Count of clients with benefits")

```





```{r}
# Now refit the full model with only complete cases
set.seed(1353)
data_split_comp <- initial_split(d.complete)
lr_train_data <- training(data_split_comp)
lr_test_data <- testing(data_split_comp)

#first global model
  fullModel <- stats::lm(formula = totserv ~ Age_ + race_white + vet
                  + race_native + eth_hispanic + males + dviolencesurvivor
                  + race2_reported, data=lr_train_data)

options(na.action = "na.fail", width=120)
#povOutput <- dredge(fullModel,  rank="AIC", fixed = NULL, extra=c("R^2",adjRsq=function(x) summary(x)$adj.r.squared))

summary(fullModel)

```

The full model results have an adjustd R-squared value of 0.1387 a p-value that is less than 0.0005.

```{r}
#povOutput <- dredge(fullModel,  rank="AIC")

MuMIn::dredge(fullModel)

kable(head(povOutput,n=15),caption="Table 6: POV Table", digits=3)

```
The global model includes variables selected using the pairs panel. Using the total count of services as the dependent variable, the following independent variables were included: Age, Race_whiet, Veteran status, Race_native, Eth_hispanic, Males, Survivors of domestic violence, and Reporting a Second Race.
Using the dredge function in the MuMin package, hundreds of models were calculated with these variables. The model with the highest AIC...
None of these models really look great. The coeffecients seem racist. 

Since the dependent variable is very skewed and is considered 'count' data, a Poisson regression model could improve the validity of a model. 

```{r}
#poisson regression
poisson_model <- glm(totserv ~ Age_  + race_native + 
    eth_hispanic + dviolencesurvivor + race2_reported, family = "poisson", data = lr_train_data)

#view model output
summary(poisson_model)


```

It is always improtant to check the assumptions in a model.
First step is to plot the residuals against the predicted values to check for homoscedasticity and linearity assumptions. 

```{r}
# Plot the residuals against the predicted values
plot(lr_train_data$totserv, residuals(poisson_model), xlab = "Predicted values", ylab = "Residuals")
abline(h = 0, lty = 2)

```
The shape of the plot shows that the data are biased and are heteroscedastic. 

Next, we can check the normality assumption of the residuals by plotting a histogram and a QQ plot of the residuals. 

```{r}
# Plot a histogram of the residuals
hist(residuals(poisson_model), main = "Histogram of residuals")

# Plot a QQ plot of the residuals
qqnorm(residuals(poisson_model))
qqline(residuals(poisson_model))
```
I think these plots show that the data is non-normal. 

Finally, we can check for multicollinearity by calculating the variance inflation factor (VIF) for each predictor variable using the car package in R

```{r}
# Load the car package
# Calculate the VIF for each predictor variable
vif(poisson_model)

```



we can conduct a Chi-Square goodness of fit test to see if the model fits the data. 


```{r}

tidy(poisson_model)

anova(poisson_model$fit)

# this one from the lecture, to measure the accuracy: 
# ```
metrics(lr_test_data,truth=totserv,estimate=poisson_model)

#chi-square to determine goodness of fit
#pchisq(19909, 271, lower.tail = FALSE)
```



##Decision Tree

```{r}
#first approach, Random Forest
# distinguish top 20 percentile in data
# Use the percentile function to calculate the percentile of "totserv"
d.tree <- d.complete %>% 
  mutate(per = ecdf(d.complete$totserv)(d.complete$totserv)) %>% 
  mutate(age = if_else(Age_ < 48, 0, 1)) %>% 
  mutate(has_income = if_else(income < 0, 0, 1))
  
# Use if_else() to create a new column based on the percentile to identify the top 20 percent
d.tree <- d.tree %>% 
  mutate(top20 = if_else(per < 0.8, 0, 1))

###drop non-binary variables
#I don't think this is necessary
#d.tree <- d.tree[, !(names(d.tree) %in% c("Age_", "months_from_start", "income", "lenstay_1", "lenstay_2to6", "lenstay_1to3mo", "lenstay_wktomo", "lenstay_1yrplus", "lenstay_3to12mo", "efood_count", "eother_count", "wfood_count", "wother_count", "totserv", "per"))]


 # Use `initial_split` to do your training and assessment splits. Use a line, like


tree_split <- initial_split(d.tree)
train_data <- training(tree_split)
test_data <- testing(tree_split)



# Set the number of trees in the forest
ntree <- 500

# Create a random forest model using the train_data data frame
rf_model <- randomForest(top20 ~ ., data = train_data, ntree = ntree)

# Print the summary of the model
print(rf_model)




```

```{r}
#Decision Tree 
#Second approach

fb.model.tree <- decision_tree() %>%  
  set_engine("rpart") %>% 
  set_mode("regression")

fb.fit.tree <- fb.model.tree %>%    
  fit(train_data$totserv ~ Age_ + insured + 
        hasincome + hasbenefit,
      data=train_data)

# add a column with the prediction 

test_data <- test_data %>% 
  bind_cols(predict(fb.fit.tree,test_data)) %>% 
  rename(pred_tree = .pred)

metrics(test_data,truth=totserv,estimate=pred_tree)
library("rpart.plot")

rpart.plot(fb.fit.tree$fit)


```





##Visualizations


```{r fig.height=8, fig.width=8,fig.cap="Figure 1: Correlation matrix with so many variables will be hard to read and some variables can be removed"}

d.pp <- d.complete[,c("totserv", "Age_",
                                  "transgender",
                                  "males", 
                                  "race_white", "lenstay_1yrplus",
                                  "lenstay_3to12mo",
                                  "race_native",
                                  "eth_hispanic", "dviolencesurvivor",
                                  "insured", "income", 
                                  "race2_reported", "hasincome"
                                  )]
pairs.panels(d.pp)
```

##Roc Curve

```{r}
######
#ROC Curve
######

# for ROC curve we need probabilities so we can sort test_data
#test_data$pred_tree <- predict(fit,test_data, type="prob")[,2] # returns prob of both cats, just need 1

roc.data <- data.frame(cutoffs = c(1,sort(unique(test_data$pred_tree),decreasing=T)),
                       TP.at.cutoff = 0,
                       TN.at.cutoff = 0)

for(i in 1:dim(roc.data)[1]){
  this.cutoff <- roc.data[i,"cutoffs"]
  roc.data$TP.at.cutoff[i] <- sum(test_data[test_data$pred_tree >= this.cutoff,"top20"] == 1)
  roc.data$TN.at.cutoff[i] <- sum(test_data[test_data$pred_tree < this.cutoff,"top20"] == 0)
}
roc.data$TPR <- roc.data$TP.at.cutoff/max(roc.data$TP.at.cutoff) 
roc.data$Specificity <- roc.data$TN.at.cutoff/max(roc.data$TN.at.cutoff) 
roc.data$FPR <- 1 - roc.data$Specificity

with(roc.data,
     plot(x=FPR,
          y=TPR,
          type = "l",
          xlim=c(0,1),
          ylim=c(0,1),
          main="ROC Curve'")     
)
abline(c(0,1),lty=2)
######End ROC code

```

##Limitations

##Recommendations

Data insights also can help government agencies and outreach workers make their operations more efficient. For instance, one of the most important outcomes of building a CES for homelessness is the ability to prioritize clients based on their vulnerability. https://www2.deloitte.com/us/en/insights/industry/public-sector/homelessness-data.html

##Conclusion



##Executive Summary
  
#Findings
#Next Steps  