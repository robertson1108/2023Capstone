---
title: "Pov_Capstone"
author: "Jamie Robertson"
date: "2023-02-20"
output: html_document
subtitle: Capstone project re The Poverello Center
bibliography: cap.bib
nocite: '@*'

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(show.signif.stars = FALSE)
library(knitr)
library(readr)
library(mosaic)
library(heplots)
library(psych)
library(car)
library(Sleuth3)
library(coneproj)
library(MuMIn)
library(rpart)
library(randomForest)
library(tidymodels)
library(car)

knitr::write_bib(c("base","rmarkdown","knitr", #don't change these
                   "NHANES","mosaic","psych","car","MuMIn"), #project specific before the close parenthesis
                   "cap.bib", width=60)

knitr::opts_chunk$set(echo = TRUE)
```


## Introduction/Background

The data source: Poverello Center
Data dictionary for HMIS data: https://files.hudexchange.info/resources/documents/HMIS-Data-Dictionary.pdf
The point of this analysis is to use benefit The Poverello Center (The Pov). This is a detail review of the data and exploratory analysis. The Pov hopes to utilize their data to identify individuals who are likely to be in the top percentile of clients. 


##Research questions

What are the most important variables to use in a model to explain high service counts a person has for services received from The Poverello Center?

Predictive risk modeling for top 20% users of homeless services in Missoula Montana. 

```{r show_col_types = FALSE}
#Read in the data

d.pov <- read_csv("C:\\Users\\jamie\\OneDrive\\Documents\\Capstone\\LRdata.csv", show_col_types = FALSE)

```
##Overview

The HMIS data from The Pov contains many demographic variables including vet status, race, income, weather or not the client holds insurance, if the client is handicapped, etc. The number of unique individuals included in the data started as 2383. There were over 50,000 service transactions. These observations were collected November 2021 through December 2022 in Missoula, Montana. The variables used in this analysis are listed explained more thoroughly in Appendix I of the associated paper. 
```{r}
head(d.pov)
```

These variables may be useful in a linear regression. The sum of services provided to a client is the dependent variable. In the cell below the counts of all services recieved by an individual and added to the dataframe. 

```{r add_counts, echo=FALSE}

d.pov <- d.pov %>% 
  mutate(totserv=efood_count+wfood_count+eother_count+wother_count)

hist(d.pov$totserv, caption="Graph 1. Histogram for summed service counts by individual")

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


##Context

The highest percentile will be the clients who stay in a homeless or at risk for a longer period of time. The top 10% consumed 45.5% of the services of the Pov in the time frame provided. 

```{r}

quantile(d.pov$totserv, probs = seq(.1, .9, by =.1))

```


##Best Practices for Data Cleaning

The data came in 4 files, a set of two from each location of the Pov. One set from the Winter Shelter which operation depends on the weather but mostly is open from November to April, one set from the Emergency Shelter that is open all year. The client files were combined and duplicates were removed in excel. A column to calculate age based on the date of birth was also entered in excel.

Many entries were missing portions of the data. After matching the Client_ID with transactions. I need to deal with so many null values. Luckily, many of the variables in the data are binary variables. Using Google Big Query and SQL were used to combine the client data with the food transaction data from the two locations. The data was transformed there to 1's and 0's. This approach turned missing data to false for the following variables: female, male, transgender, veteran status, race_white, race_black, race_native, race_asian, ethnically hispanic, domestic violence survivor, covered by health insurance, has a disabling condition, if they have stayed one night, two to six nights, one week to a month, one to three months, three months to a year, or a year and over, if they are the head of household, reported a second race, has income from any source, or recieves a non-cash benefit.  This code can be found in the appendix of the associated paper. 

The variable that holds the total number of months an individual has been homeless contains many null values. This variable was removed and complete cases were consolidated. There are 347 complete rows of data.  



```{r}

#Deal with null values
d <-   select(d.pov, Age_,
         females, 
         transgender, 
         males, 
         vet, 
         race_white, 
         race_black, 
         race_native,
         race_asian, 
         eth_hispanic, 
         dviolencesurvivor, 
         insured, 
         disabled, 
         lenstay_1, 
         lenstay_2to6, 
         lenstay_1to3mo, 
         lenstay_wktomo, 
         lenstay_1yrplus, 
         lenstay_3to12mo, 
         income, 
         headofhouse, 
         race2_reported, 
         hasincome, 
         hasbenefit, 
         totserv) 
         
d.complete.sum <- complete.cases(d)
summary(d.complete.sum)

d.complete <- d[complete.cases(d),]

```    


#### Data Summaries

As noted above and described below in Table 1, the quantitative variables are age,.... The standard deviation of TotServ is 1.062, the mean is 5.053, and the shape is right skewed. The count of catagorical variables

```{r}
variableNames <- c("Age_", "months_from_start", "income","efood_count","eother_count",
                   "wfood_count","wother_count","totserv")
kable(describe(d.pov[,variableNames])[,-c(1,6:7,11:12)], digits=3,
      caption="Table 1. Summary Statistics of Quantitative Variables")
```



##Code explanations



The point of this analysis is to use Akaike's "An Information Criterion" method to select the appropriate model to answer the research question.  This method is also known as AIC model selection.  
The point of the analysis is to determine the most important variables to use in a model to explain the amount of services used by a client. This paper reviews some descriptive statistics from the data that utilizes AIC to pick the best model.


Total Count of Services will be the response variable.  This variable will be identified as TotServ and measures a client consumption of services. This is a quantitative variable. 


```{r}

kable(tally(d.pov$Age_), caption = "Table 2: Count of client Ages")
#tried to change names
#kable(tally(d.pov$Age_, ""  ), caption = "Table 2: Count of clients ages")

```





```{r}
# Now refit the full model with only complete cases
set.seed(1353)
data_split_comp <- initial_split(d.complete)
lr_train_data <- training(data_split_comp)
lr_test_data <- testing(data_split_comp)

#first global model
  fullModel <- stats::lm(formula = totserv ~ Age_ + race_white + vet
                  + race_native + eth_hispanic + males + dviolencesurvivor
                  + race2_reported, data=lr_train_data)

options(na.action = "na.fail", width=120)
#povOutput <- dredge(fullModel,  rank="AIC", fixed = NULL, extra=c("R^2",adjRsq=function(x) summary(x)$adj.r.squared))

summary(fullModel)

```

The full model results have an adjustd R-squared value of 0.1387 a p-value that is less than 0.0005.

```{r}
povOutput <- dredge(fullModel,  rank="AIC")

MuMIn::dredge(fullModel)

kable(head(povOutput,n=15),caption="Table 6: POV Table", digits=3)

```
The global model includes variables selected using the pairs panel. Using the total count of services as the dependent variable, the following independent variables were included: Age, Race_whiet, Veteran status, Race_native, Eth_hispanic, Males, Survivors of domestic violence, and Reporting a Second Race.
Using the dredge function in the MuMin package, hundreds of models were calculated with these variables. The model with the highest AIC...
None of these models really look great. The coeffecients seem racist. 

Since the dependent variable is very skewed and is considered 'count' data, a Poisson regression model could improve the validity of a model. 

```{r}
#poisson regression
poisson_model <- glm(totserv ~ Age_  + race_native + 
    eth_hispanic + dviolencesurvivor + race2_reported, family = "poisson", data = lr_train_data)

#view model output
summary(poisson_model)


```

It is always improtant to check the assumptions in a model.
First step is to plot the residuals against the predicted values to check for homoscedasticity and linearity assumptions. 

```{r}
# Plot the residuals against the predicted values
plot(lr_train_data$totserv, residuals(poisson_model), xlab = "Predicted values", ylab = "Residuals")
abline(h = 0, lty = 2)

```
The shape of the plot shows that the data are biased and are heteroscedastic. 

Next, we can check the normality assumption of the residuals by plotting a histogram and a QQ plot of the residuals. 

```{r}
# Plot a histogram of the residuals
hist(residuals(poisson_model), main = "Histogram of residuals")

# Plot a QQ plot of the residuals
qqnorm(residuals(poisson_model))
qqline(residuals(poisson_model))
```
I think these plots show that the data is non-normal. 

Finally, we can check for multicollinearity by calculating the variance inflation factor (VIF) for each predictor variable using the car package in R

```{r}
# Load the car package
# Calculate the VIF for each predictor variable
vif(poisson_model)

```



we can conduct a Chi-Square goodness of fit test to see if the model fits the data. 


```{r}

tidy(poisson_model)



#chi-square to determine goodness of fit
pchisq(19909, 271, lower.tail = FALSE)
```



##Decision Tree

```{r}

#Data Transformation
#Add percentage variable
#Deal with null values

d.pov <- d.pov %>% 
  mutate(per = ecdf(d.pov$totserv)(d.pov$totserv))

d.rf <-   select(d.pov, Age_,
         females, 
         transgender, 
         males, 
         vet, 
         race_white, 
         race_black, 
         race_native,
         race_asian, 
         eth_hispanic, 
         dviolencesurvivor, 
         insured, 
         disabled, 
         income, 
         headofhouse, 
         race2_reported, 
         hasincome, 
         hasbenefit,
         per) 
         
rf.complete.sum <- complete.cases(d.rf)
summary(rf.complete.sum)

d.tree<- d.rf[complete.cases(d.rf),]



library(randomForestExplainer)

```

```{r}

 # Use `initial_split` to do your training and assessment splits. 
set.seed(2023)

tree_split <- initial_split(d.tree)
train_data <- training(tree_split)
test_data <- testing(tree_split)

```

```{r}
#Create forest and print call 

forest <- randomForest(per ~ ., data = train_data, localImp = TRUE)

forest
```

Try to extract top performing tree. 

```{r}
# Extract individual trees from the forest
trees <- forest$forest

# Evaluate each tree's performance on a validation set
mse <- rep(NA, length(trees))
for (i in seq_along(trees)) {
  tree_pred <- predict(trees[[i]], newdata = test_data)
  mse[i] <- mean((test_data$per - tree_pred)^2)
}

# Find the index of the best-performing tree
best_tree_index <- which.min(mse)

# Extract the best-performing tree from the forest
best_tree <- trees[[best_tree_index]]
```



To obtain the distribution of minimal depth we pass our forest to the function min_depth_distribution and store the result, which contains the following columns (we save this and load it from memory as it takes a while):


```{r}

min_depth_frame <- min_depth_distribution(forest)
save(min_depth_frame, file = "min_depth_frame.rda")

load("min_depth_frame.rda")
head(min_depth_frame, n = 10)

```


Next, we pass it to the function plot_min_depth_distribution and under default settings obtain obtain a plot of the distribution of minimal depth for top ten variables according to mean minimal depth calculated using top trees (mean_sample = "top_trees"). We could also pass our forest directly to the plotting function but if we want to make more than one plot of the minimal depth distribution is more efficient to pass the min_depth_frame to the plotting function so that it will not be calculated again for each plot (this works similarly for other plotting functions of randomForestExplainer).

```{r}
# plot_min_depth_distribution(forest) # gives the same result as below but takes longer
plot_min_depth_distribution(min_depth_frame)
```

To further explore variable importance measures we pass our forest to measure_importance function and get the following data frame 

```{r}
importance_frame <- measure_importance(forest)
save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
importance_frame
```

Below we present the result of plot_multi_way_importance for the default values of x_measure and y_measure, which specify measures to use on x
 and y-axis, and the size of points reflects the number of nodes split on the variable. For problems with many variables we can restrict the plot to only those used for splitting in at least min_no_of_trees trees. By default 10 top variables in the plot are highlighted in blue and labeled (no_of_labels) – these are selected using the function important_variables, i.e. using the sum of rankings based on importance measures used in the plot (more variables may be labeled if ties occur).

```{r}
# plot_multi_way_importance(forest, size_measure = "no_of_nodes") # gives the same result as below but takes longer
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")
```



```{r}
plot_multi_way_importance(importance_frame, x_measure = "mse_increase", y_measure = "node_purity_increase", size_measure = "p_value", no_of_labels = 5)

```


Compare measures using ggpairs
Generally, the multi-way importance plot offers a wide variety of possibilities so it can be hard to select the most informative one. One idea of overcoming this obstacle is to first explore relations between different importance measures to then select three that least agree with each other and use them in the multi-way importance plot to select top variables. The first is easily done by plotting selected importance measures pairwise against each other using plot_importance_ggpairs as below. One could of course include all seven measures in the plot but by default p-value and the number of trees are excluded as both carry similar information as the number of nodes.

```{r}
# plot_importance_ggpairs(forest) # gives the same result as below but takes longer
plot_importance_ggpairs(importance_frame)

```


Compare different rankings
In addition to scatter plots and correlation coefficients, the ggpairs plot also depicts density estimate for each importance measure – all of which are in this case very skewed. An attempt to eliminate this feature by plotting rankings instead of raw measures is implemented in the function plot_importance_rankings that also includes the fitted LOESS curve in each plot.

```{r}
# plot_importance_rankings(forest) # gives the same result as below but takes longer
plot_importance_rankings(importance_frame)
```

Variable interactions
Conditional minimal depth
After selecting a set of most important variables we can investigate interactions with respect to them, i.e. splits appearing in maximal subtrees with respect to one of the variables selected. To extract the names of 5 most important variables according to both the mean minimal depth and number of trees in which a variable appeared, we pass our importance_frame to the function important_variables as follows:

```{r}
# (vars <- important_variables(forest, k = 5, measures = c("mean_min_depth", "no_of_trees"))) # gives the same result as below but takes longer
(vars <- important_variables(importance_frame, k = 5, measures = c("mean_min_depth", "no_of_trees")))
```
We pass the result together with or forest to the min_depth_interactions function to obtain a data frame containing information on mean conditional minimal depth of variables with respect to each element of vars (missing values are filled analogously as for unconditional minimal depth, in one of three ways specified by mean_sample). If we would not specify the vars argument then the vector of conditioning variables would be by default obtained using important_variables(measure_importance(forest)).

```{r}
interactions_frame <- min_depth_interactions(forest, vars)
save(interactions_frame, file = "interactions_frame.rda")
load("interactions_frame.rda")
head(interactions_frame[order(interactions_frame$occurrences, decreasing = TRUE), ])
```

Then, we pass our interactions_frame to the plotting function plot_min_depth_interactions and obtain the following:

```{r}
# plot_min_depth_interactions(forest) # calculates the interactions_frame for default settings so may give different results than the function below depending on our settings and takes more time
plot_min_depth_interactions(interactions_frame)
```

Prediction of the forest on a grid
To further investigate the most frequent interaction lstat:rm we use the function plot_predict_interaction to plot the prediction of our forest on a grid of values for the components of each interaction. The function requires the forest, training data, variable to use on x
 and y
-axis, respectively. In addition, one can also decrease the number of points in both dimensions of the grid from the default of 100 in case of insufficient memory using the parameter grid.
```{r}
plot_predict_interaction(forest, train_data, "race2_reported", "income")
```


Explain the forest
The explain_forest() function is the flagship function of the randomForestExplainer package, as it takes your random forest and produces a html report that summarizes all basic results obtained for the forest with the new package.

```{r}
explain_forest(forest, interactions = TRUE, data = train_data)
```


##Visualizations


```{r fig.height=8, fig.width=8,fig.cap="Figure 1: Correlation matrix with so many variables will be hard to read and some variables can be removed"}

d.pp <- d.complete[,c("totserv", "Age_",
                                  "transgender",
                                  "males", 
                                  "race_white", "lenstay_1yrplus",
                                  "lenstay_3to12mo",
                                  "race_native",
                                  "eth_hispanic", "dviolencesurvivor",
                                  "insured", "income", 
                                  "race2_reported", "hasincome"
                                  )]
pairs.panels(d.pp)
```

##Roc Curve

```{r}
######
#ROC Curve
######

# for ROC curve we need probabilities so we can sort test_data
#test_data$pred_tree <- predict(fit,test_data, type="prob")[,2] # returns prob of both cats, just need 1

roc.data <- data.frame(cutoffs = c(1,sort(unique(test_data$pred_tree),decreasing=T)),
                       TP.at.cutoff = 0,
                       TN.at.cutoff = 0)

for(i in 1:dim(roc.data)[1]){
  this.cutoff <- roc.data[i,"cutoffs"]
  roc.data$TP.at.cutoff[i] <- sum(test_data[test_data$pred_tree >= this.cutoff,"top20"] == 1)
  roc.data$TN.at.cutoff[i] <- sum(test_data[test_data$pred_tree < this.cutoff,"top20"] == 0)
}
roc.data$TPR <- roc.data$TP.at.cutoff/max(roc.data$TP.at.cutoff) 
roc.data$Specificity <- roc.data$TN.at.cutoff/max(roc.data$TN.at.cutoff) 
roc.data$FPR <- 1 - roc.data$Specificity

with(roc.data,
     plot(x=FPR,
          y=TPR,
          type = "l",
          xlim=c(0,1),
          ylim=c(0,1),
          main="ROC Curve'")     
)
abline(c(0,1),lty=2)
######End ROC code

```

##Limitations

##Recommendations

Data insights also can help government agencies and outreach workers make their operations more efficient. For instance, one of the most important outcomes of building a CES for homelessness is the ability to prioritize clients based on their vulnerability. https://www2.deloitte.com/us/en/insights/industry/public-sector/homelessness-data.html

##Conclusion



##Executive Summary
  
#Findings
#Next Steps  